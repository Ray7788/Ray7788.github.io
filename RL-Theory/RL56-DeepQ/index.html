<!DOCTYPE html>
<html lang="en">
    <head prefix="og: https://ogp.me/ns#">
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="color-scheme" content="light dark">
  
  <title>RL56-DeepQ - Ray</title>
  
    <link rel="shortcut icon" href="/source/personal/rui.jpg">
  
  
    <link rel='manifest' href='/manifest.json'>
  

  
  
  
  <meta property="og:title" content="RL56-DeepQ - Ray" />
  
  <meta property="og:type" content="article" />
  
  <meta property="og:url" content="http://example.com/RL-Theory/RL56-DeepQ/index.html" />
  
  <meta property="og:image" content="/source/personal/rui.jpg" />
  
  <meta property="og:article:published_time" content="2023-12-31T20:15:19.036Z" />
  
  <meta property="og:article:author" content="Rui Xu" />
  
  

  
<link rel="stylesheet" href="/css/var.css">

  
<link rel="stylesheet" href="/css/main.css">

  
<link rel="stylesheet" href="/css/typography.css">

  
<link rel="stylesheet" href="/css/code-highlighting.css">

  
<link rel="stylesheet" href="/css/components.css">

  
<link rel="stylesheet" href="/css/nav.css">

  
<link rel="stylesheet" href="/css/paginator.css">

  
<link rel="stylesheet" href="/css/footer.css">

  
<link rel="stylesheet" href="/css/post-list.css">

  
  
<link rel="stylesheet" href="/css/rainbow-banner.css">

  
  
  
<link rel="stylesheet" href="/css/toc.css">

  
  
  
  
  
<link rel="stylesheet" href="/css/post.css">

  
  
  
  
  

  
<meta name="generator" content="Hexo 6.3.0"></head>
    <body
        data-color-scheme="auto"
        data-uppercase-categories="true"
        
        data-rainbow-banner="true"
        data-rainbow-banner-shown="auto"
        data-rainbow-banner-month="6"
        data-rainbow-banner-colors="#e50000,#ff8d00,#ffee00,#008121,#004cff,#760188"
        
        data-config-root="/"
        
        data-toc="true"
        data-toc-max-depth="2"
        
        
    >
        <nav id="theme-nav">
    <div class="inner">
        <a class="title" href="/">Ray</a>
        <div class="nav-arrow"></div>
        <div class="nav-items">
            <a class="nav-item nav-item-home" href="/">Home</a>
            
            
            <a class="nav-item" href="/archives">Archives</a>
            
            
            
            <a class="nav-item" href="/friends">Friends</a>
            
            
            
            <a class="nav-item" href="/projects">Projects</a>
            
            
            
            <a class="nav-item" href="/about">About</a>
            
            
            
            <a class="nav-item nav-item-github nav-item-icon" href="https://github.com/Ray7788" target="_blank" aria-label="GitHub">&nbsp;</a>
            
            
            
            <a class="nav-item nav-item-search nav-item-icon" href="/search" target="_blank" aria-label="Search">&nbsp;</a>
            
            
        </div>
    </div>
</nav>
        
<article class="post">
    <div class="meta">
        
        <div class="categories text-uppercase">
        
            <a href="/categories/RL-Theory/">RL-Theory</a>
        
        </div>
        

        

        <h1 class="title">RL56-DeepQ</h1>
    </div>

    <div class="divider"></div>

    <div class="content">
        <h1 id="前情提要"><a href="#前情提要" class="headerlink" title="前情提要"></a>前情提要</h1><ul>
<li><p>Sarsa和Q-learning在探索时都是用到了ε-greedy的策略。在训练过程中我们既要保证我们会沿着Q表格的指引进行活动，但同时我们有需要进行探索，我们不能固守已有的Q表格而不做出新的尝试，如何确定某一步是按照Q表格进行还是进行一步随机探索呢，这里使用到了ε-greedy，我们会事先设定一个ε的值，假设我设定ε值为(0,1)，在每一部行动前我们会掷色子（在0-1之间生成一个随机数），随机数如果大于ε我们选择按Q表格行动，小于ε我们会在可能的行动中随机探索一次。</p>
</li>
<li><p>区分on-policy和off-policy</p>
</li>
<li>MDP包括{S,A,R,P}四个要素，其中R为奖励函数，P为环境的状态转移概率，即环境不确定度，但是以上两个因素很多情况下是未知的，这时就需要使用无模型（model-free）的方法。</li>
</ul>
<h1 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h1><p>Q-Learning的目的是学习特定State下、特定Action的价值。Q-table可以形象地将其称为生活手册，因为其表示了在不同动作和状态之下所得到的Q函数.建立一个Q-Table，以State为行、Action为列，通过每个动作带来的奖赏更新Q-Table。因此,Q-table可以作为一种我们强化学习之后得到的结果，也就是输出。</p>
<ul>
<li>e_predict</li>
<li>e_target</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">初始化:	初始化学习率、可执行动作、Q_table等参数</span><br><span class="line">动作选择:	根据小男孩当前所处的环境和Q_table进行动作选择</span><br><span class="line">学习:	根据小男孩当前所处的环境对其它环境的预测情况q_predict和下一步环境的实际情况q_target更新Q_table表</span><br><span class="line">确认是否存在该环境:	由于在学习之前环境是未知的，当进入一个新环境时，需要生成一个得分都为0的动作表格</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">模块名称	作用/功能</span><br><span class="line">初始化	初始化环境参数、用于构建环境</span><br><span class="line">图画更新	用于更新当前的图画，便于用户观察</span><br><span class="line">环境观察	用于返回当前环境情况</span><br><span class="line">终点观察	用于返回是否到达终点</span><br><span class="line">更新坐标	用于更新当前所处位置</span><br><span class="line">下一环境获取	用于获取下一步的环境的实际情况</span><br><span class="line">参数归零	用于每一个世代坐标和当前行走步数的归零</span><br></pre></td></tr></table></figure>
<p>Q-Learning是off-policy的。异策略是指行动策略和评估策略不是一个策略。Q-Learning中行动策略是ε-greedy策略，要更新Q表的策略是贪婪策略。</p>
<p>①target policy：目标策略，如同战役中的军师，在幕后学习前方将士传来的经验并做出决策，不需要亲自与环境交互，是我们要学习的策略。</p>
<p>②behavior policy：行为策略，如同一线的战士，根据军师的命令在战场上实际作战，并将得到的经验传给后方的军师。</p>
<p> $Q=reward_1  +γ∗reward _2 +…+γ∗reward_n$<br>​<br>我们先看下Q-learning算法的更新Q值的公式：</p>
<p> <strong>$Q∗(s_t,a_t)=Q(s_t ,a_t)+α[R_t+γ a ′max(Q(s_t+1 ,a_t+1)−Q(s_t,a_t )]$</strong></p>
<p><img src="https://img-blog.csdnimg.cn/a5d89c06bd624a9295316c0346f73711.png#pic_center" alt="算法伪代码"></p>
<h1 id="SARSA"><a href="#SARSA" class="headerlink" title="SARSA"></a>SARSA</h1><p>Sarsa全称是<strong>state-action-reward-state-action</strong>。 也是采用Q-table的方式存储动作值函数；而且决策部分和Q-Learning是一样的, 也是采用ε-greedy策略。不同的地方在于 Sarsa 的更新方式是不一样的。</p>
<p>Q-Learning算法，先假设下一步选取最大奖赏的动作，更新值函数。然后再通过ε-greedy策略选择动作。<br>Sarsa算法，先通过ε-greedy策略执行动作，然后根据所执行的动作，更新值函数。</p>
<p>我们再看下Sarsa算法的更新Q值的公式</p>
<p> <strong>$Q∗(s<em>t,a_t)=Q(s_t ,a_t)+α[R_t+γ(Q(s</em>(t+1) ,a_(t+1))−Q(s_t,a_t )]$</strong></p>
<p><img src="https://img-blog.csdnimg.cn/2a37344f3db347deb8d3f2b81fbdf22d.png#pic_center" alt="算法伪代码"></p>
<p>因此两种算法在表现上也会有很大的区别，Sarsa表现得更为胆小，因为它会记住每一次错误的探索，它会对错误较为敏感，而Q-learning只在乎Q值的最大化，因此Q-learning会十分贪婪，表现得十分aggressive。</p>
<h1 id="推荐阅读"><a href="#推荐阅读" class="headerlink" title="推荐阅读"></a>推荐阅读</h1><p><a target="_blank" rel="noopener" href="http://t.csdnimg.cn/F0DZT">http://t.csdnimg.cn/F0DZT</a><br><a target="_blank" rel="noopener" href="http://t.csdnimg.cn/98Ttu">http://t.csdnimg.cn/98Ttu</a></p>

    </div>

    
    <div class="about">
        <h1>About this Post</h1>
        <div class="details">
            <p>This post is written by Rui Xu, licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc/4.0">CC BY-NC 4.0</a>.</p>
        </div>
        
        <p class="tags">
            
            <i class="icon"></i>
            <a href="/tags/AI/" class="tag">#AI</a><a href="/tags/Reinforcement-Learning/" class="tag">#Reinforcement Learning</a>
        </p>
        
    </div>
    

    <div class="container post-prev-next">
        
        <a href="/uncategorized/RL5-TD/" class="next">
            <div>
                <div class="text">
                    <p class="label">Next</p>
                    <h3 class="title">RL5-TD</h3>
                </div>
            </div>
        </a>
        
        
        <a href="/Prepare/Set-up-machine/" class="prev">
            <div>
                <div class="text">
                    <p class="label">Previous</p>
                    <h3 class="title">Set up DL machine配置深度学习环境</>
                </div>
            </div>
        </a>
        
    </div>

    
</article>

        <footer>
    <div class="inner">
        <div class="links">
            
            <div class="group">
                <h2 class="title">Blog</h2>
                
                <a href="/" class="item">Blog</a>
                
                <a href="/archives" class="item">Archives</a>
                
                <a href="/tags" class="item">Tags</a>
                
                <a href="/categories" class="item">Categories</a>
                
                <a href="/search" class="item">Search</a>
                
                <a href="/friends" class="item">Friends</a>
                
                <a href="/projects" class="item">Projects</a>
                
                <a href="/about" class="item">About</a>
                
                <a href="/atom.xml" class="item">RSS</a>
                
            </div>
            
            <div class="group">
                <h2 class="title">Projects</h2>
                
            </div>
            
            <div class="group">
                <h2 class="title">Me</h2>
                
                <a target="_blank" rel="noopener" href="https://github.com/Ray7788" class="item">GitHub</a>
                
                <a href="ray778@foxmail.com" class="item">Email</a>
                
            </div>
            
        </div>
        <span>&copy; 2024 Rui Xu<br>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a> </span>
        
        <br>
        <span class="footer-extra-description">永远相信美好的事情即将发生。Always believe something great is going to happen.</span>
        
        
            <br>
            <div class="color-scheme-toggle" role="radiogroup" id="theme-color-scheme-toggle">
                <label>
                    <input type="radio" value="light">
                    <span>Light</span>
                </label>
                <label>
                    <input type="radio" value="dark">
                    <span>Dark</span>
                </label>
                <label>
                    <input type="radio" value="auto">
                    <span>Auto</span>
                </label>
            </div>
        
    </div>
</footer>


        
<script src="/js/main.js"></script>

        
        
        

        
        <script src="https://unpkg.com/scrollreveal"></script>
        <script>
            window.addEventListener('load', () => {
                ScrollReveal({ delay: 250, reset: true, easing: 'cubic-bezier(0, 0, 0, 1)' })
                ScrollReveal().reveal('.post-list-item .cover-img img')
                ScrollReveal().reveal('.post-list-item, .card, .content p img, .content .block-large img', { distance: '60px', origin: 'bottom', duration: 800 })
            })
        </script>
        
    </body>
</html>